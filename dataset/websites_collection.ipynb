{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c12b07",
   "metadata": {},
   "source": [
    "## 1. Inputs: Two URL Lists\n",
    "\n",
    "* **Phishing URLs (label = 1)**\n",
    "  It expects a CSV (by default named `verified_online.csv`) with the PhishTank data. That file has columns like\n",
    "  `phish_id, url, phish_detail_url, …, online, target`.\n",
    "  When you call\n",
    "\n",
    "  ```python\n",
    "  process_urls_threaded(\n",
    "      \"verified_online.csv\",      # input_file\n",
    "      \"phishing_websites.csv\",    # output_file\n",
    "      label=1,                    # tells it “these are phishing”\n",
    "      …\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  it reads those columns (including the `url` field) into a DataFrame, takes the `url` column, and uses those as the list of pages to visit.\n",
    "\n",
    "* **Legitimate URLs (label = 0)**\n",
    "  It expects a simple CSV of popular domains (by default `tranco_list.csv`) with no header, just rank and domain name. When called with `label=0`, it reads each domain, prefixes it with `http://`, and uses that as the list of “legitimate” pages to visit.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feature Extraction Worker\n",
    "\n",
    "The heart of the script is a pool of worker threads that:\n",
    "\n",
    "1. **Initialize a headless Chrome driver** (via Selenium) with images and notifications disabled.\n",
    "2. **For each URL** (phishing or legit):\n",
    "\n",
    "   * Calls `driver.get(url)` (with a 2 s timeout).\n",
    "   * Grabs the rendered page HTML (`driver.page_source`) into BeautifulSoup.\n",
    "   * Passes `driver`, the `soup`, and the `url` into the project’s `FeaturesExtraction` class.\n",
    "   * Calls `create_vector()`, which returns a list of feature values (all the URL‑, HTML‑ and dynamic‑interaction features).\n",
    "   * **Appends** the raw URL string and the label (0 or 1) to that feature vector.\n",
    "   * **Immediately writes** that single row into the output CSV (either `phishing_websites.csv` or `legitimate_websites.csv`) in append mode, under a thread‑safe lock, so you end up with one big features+label table.\n",
    "\n",
    "Any time a page times out or throws an exception, it logs the URL and continues.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Outputs\n",
    "\n",
    "* **`phishing_websites.csv`**\n",
    "  A CSV whose columns are exactly the 73 features returned by `get_feature_columns()`, plus two final columns:\n",
    "\n",
    "  * `URL` (the string you visited)\n",
    "  * `label` (1 for phishing)\n",
    "\n",
    "* **`legitimate_websites.csv`**\n",
    "  Same format, but `label` is 0.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to Run It\n",
    "\n",
    "At the bottom of the file there’s an `if __name__ == \"__main__\":` block where they set:\n",
    "\n",
    "```python\n",
    "PHISH_THREADS = 32\n",
    "LEGITIMATE_THRESHOLD = 16\n",
    "\n",
    "# Example (commented out) call for phishing:\n",
    "# process_urls_threaded(\n",
    "#   \"verified_online.csv\", \"phishing_websites.csv\", label=1,\n",
    "#   start_row=21000, end_row=22000, num_threads=PHISH_THREADS,\n",
    "# )\n",
    "\n",
    "# Active call for legit:\n",
    "process_urls_threaded(\n",
    "  \"tranco_list.csv\", \"legitimate_websites.csv\", label=0,\n",
    "  start_row=24100, end_row=24300, num_threads=LEGITIMATE_THRESHOLD,\n",
    ")\n",
    "```\n",
    "\n",
    "You would typically:\n",
    "\n",
    "1. Pick your row range (`start_row`, `end_row`) to control how many URLs to process.\n",
    "2. Decide how many threads to use.\n",
    "3. Run the script twice: once with `label=1` on the PhishTank CSV (to build `phishing_websites.csv`), then with `label=0` on the Tranco CSV (to build `legitimate_websites.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "* **Inputs**: live lists of URLs (one phishing, one legit).\n",
    "* **Processing**: headless browser + feature‑extraction class.\n",
    "* **Outputs**: two large CSVs of feature vectors ready for model training.\n",
    "\n",
    "Once you’ve generated those CSVs, you can load them in your notebook (or Python script) and begin training and evaluating your classifiers. That’s the foundation of the entire project’s “URL + content → feature vector → model” pipeline.\n",
    "\"\"\"\n",
    "\n",
    "# URL & Content-based Phishing Detection Pipeline\n",
    "\"\"\"\n",
    "**Purpose:**\n",
    "This script reads lists of URLs (either verified phishing sites or top-ranked legitimate domains), loads each page in a headless browser,\n",
    " extracts a rich set of features (both URL‐based and HTML/JavaScript‐based), and writes them as feature vectors into a CSV. \n",
    "These vectors can then be used to train or evaluate machine learning models to distinguish phishing pages from legitimate ones.\n",
    "\n",
    "**Key Components & Theory:**\n",
    "1. **Feature Engineering:**\n",
    "   - **URL Features:** Length, presence of IP address, number of subdomains, redirection behavior, age of domain, etc.\n",
    "   - **HTML/JS Features:** Number of forms, input fields, scripts, hidden elements, use of popups, monitoring scripts (e.g., keylogging, clipboard access), etc.\n",
    "   Good feature selection is critical: combining structural clues (URL patterns) with behavioral indicators (DOM/JS characteristics) often yields higher detection accuracy.\n",
    "\n",
    "2. **Selenium + BeautifulSoup:**\n",
    "   - **Selenium** automates a real browser, enabling execution of dynamic JavaScript and rendering complex pages.\n",
    "   - **BeautifulSoup** parses the final rendered HTML for static analysis.\n",
    "   This hybrid approach captures both static and dynamic content cues.\n",
    "\n",
    "3. **Multithreading & I/O Bound Tasks:**\n",
    "   - Loading pages over the network is I/O bound. Using a thread pool speeds up the crawl by overlapping downloads.\n",
    "   - A `Queue` distributes URLs to worker threads; a `Lock` ensures thread‐safe writes to the CSV.\n",
    "\n",
    "4. **Scalability Considerations:**\n",
    "   - Headless mode and disabling images/notifications reduce resource usage.\n",
    "   - Timeouts prevent hanging on slow or unresponsive sites.\n",
    "   - Thread‐local counters track progress per thread without race conditions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   Let’s break it down step by step in everyday language—no jargon:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What this script does, in a nutshell\n",
    "\n",
    "1. **Reads** a list of website addresses (URLs).\n",
    "2. **Opens** each page in a “headless” browser (a browser you can’t see) to let JavaScript run.\n",
    "3. **Scans** the finished page for lots of clues (features) that hint at phishing or not.\n",
    "4. **Saves** each page’s clues as one row in a CSV file.\n",
    "5. **Repeats** this quickly by running many mini‑workers (threads) at once.\n",
    "\n",
    "\n",
    "\n",
    "### Why it’s built this way\n",
    "\n",
    "* **Speed:** Threads + headless browser + disabled images = faster crawling.\n",
    "* **Safety:** Timeouts and exception catches mean one bad URL won’t crash everything.\n",
    "* **Scalability:** You can increase threads or split input CSVs into chunks.\n",
    "* **Modularity:** All feature logic lives in `FeaturesExtraction`, so the crawling code stays clean.\n",
    "\n",
    "Hope this clears up how each piece fits together! Let me know if any part still feels fuzzy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94490e4",
   "metadata": {},
   "source": [
    "# Imports & Global Variables\n",
    "\n",
    "* **threading**:allows the script to run multiple operations at the same time (threads), so you can process many URLs in parallel, making the crawl much faster.\n",
    "* **Queue**: is a thread-safe data structure that lets threads share work (URLs to process) without interfering with each other or causing data corruption. A queue in Python is a linear data structure that follows the First-In, First-Out (FIFO) principle, meaning the first element added to the queue is the first one to be removed.\n",
    "* **selenium**: Selenium is a powerful tool for automating browsers.\n",
    "    * It launches a real Chrome browser (invisible/headless mode), loads web pages, and interacts with them just like a human would.\n",
    "    * This is crucial for pages that use JavaScript, as it ensures all dynamic content is loaded before analysis.\n",
    "    * **webdriver**  launch and interact with a browser session so that web pages can be fully loaded and analyzed.\n",
    "\n",
    "* **BeautifulSoup**: BeautifulSoup is a Python library for parsing HTML and XML documents.\n",
    "      * After Selenium loads a page, BeautifulSoup reads the HTML so you can easily extract and count elements like forms, buttons, links, and more.\n",
    "      * It’s especially useful for web scraping and analyzing the structure of web pages.\n",
    "* **pandas**: reads and writes CSV files where all our results get stored.\n",
    "* **tqdm**: shows a progress bar so you know how far along we are.\n",
    "* **url warnings**: turns off annoying SSL warnings so they don’t clutter our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f44ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FeaturesExtraction' from 'features_extraction' (c:\\Users\\USER\\Desktop\\MLproject\\ayman\\dataset\\features_extraction.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m  \u001b[38;5;66;03m# pandas for reading/writing CSVs and DataFrame operations\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m  \u001b[38;5;66;03m# for checking file existence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeatures_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturesExtraction  \u001b[38;5;66;03m# custom module to extract phishing features\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     InsecureRequestWarning,\n\u001b[32m     17\u001b[39m )  \u001b[38;5;66;03m# warning for insecure HTTPS requests\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_warnings  \u001b[38;5;66;03m# function to suppress urllib3 warnings\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FeaturesExtraction' from 'features_extraction' (c:\\Users\\USER\\Desktop\\MLproject\\ayman\\dataset\\features_extraction.py)"
     ]
    }
   ],
   "source": [
    "import threading  \n",
    "from queue import Queue  \n",
    "from bs4 import BeautifulSoup  # Beautiful Soup is a Python library designed for parsing HTML and XML documents, including those with malformed or imperfect markup. It facilitates the extraction of data from these documents, making it a popular tool for web scraping. \n",
    "\n",
    "from selenium import webdriver  #  to write Python scripts that control a web browser, navigate to web pages, and interact with elements on those pages\n",
    "from selenium.webdriver.chrome.options import (\n",
    "    Options,\n",
    ")  # to configure ChromeDriver options\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    ")  # exception for page load timeouts\n",
    "import pandas as pd  # pandas for reading/writing CSVs and DataFrame operations\n",
    "import os  # for checking file existence\n",
    "from features_extraction import FeaturesExtraction  # custom module to extract phishing features\n",
    "from urllib3.exceptions import (\n",
    "    InsecureRequestWarning,\n",
    ")  # warning for insecure HTTPS requests\n",
    "from urllib3 import disable_warnings  # function to suppress urllib3 warnings\n",
    "from tqdm import tqdm  # progress bar utility for iterables\n",
    "\n",
    "disable_warnings(InsecureRequestWarning)  # suppress insecure request warnings globally\n",
    "\n",
    "# Thread-local storage for counters to avoid race conditions\n",
    "phishing_count = threading.local()  # Each thread gets its own 'phishing_count.value'\n",
    "legitimate_count = threading.local()  # Each thread gets its own 'legitimate_count.value'\n",
    "\n",
    "# What this means:\n",
    "# Instead of using a single global variable (which could be changed by multiple threads at once and cause errors),\n",
    "# we use threading.local() to give each thread its own separate variable.\n",
    "# This is important because when running many threads at the same time (to process URLs faster),\n",
    "# we want each thread to keep track of its own progress without interfering with others.\n",
    "# Threads allow us to do many tasks (like loading web pages) in parallel, making the whole process much faster.\n",
    "# threading.local() gives each thread its own independent variable.\n",
    "# So, when a thread sets phishing_count.value = 1, it doesn't affect the value in other threads.\n",
    "# This helps keep the count accurate and prevents bugs when multiple threads are running at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad92cc",
   "metadata": {},
   "source": [
    "#  Browser Initialization\n",
    "\n",
    "**Purpose:** Encapsulate headless Chrome setup so each worker can spin up its own browser.\n",
    "\n",
    "- Runs Chrome in headless mode (no visible window).  \n",
    "- Disables sandbox and shared‑memory usage for container compatibility.  \n",
    "- Turns off images & notifications for faster page loads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4570f",
   "metadata": {},
   "source": [
    "## 3. Setting up the browser (`initialize_driver`)\n",
    "\n",
    "* We ask Chrome to run in **headless** mode (no visible window).\n",
    "* We disable images and pop‑up notifications so pages load faster and use less memory.\n",
    "* We return a ready‑to‑go Chrome instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60577c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Creates and returns a Chrome browser that runs invisibly in the background.\n",
    "    This function sets up Chrome with specific options to make it work better for web scraping:\n",
    "    - Opens Chrome in 'headless' mode (no window pops up).\n",
    "    - Turns off images and pop-up notifications so pages load faster.\n",
    "    - Tweaks settings so it works smoothly in virtual machines or Docker.\n",
    "    - Gives you back a browser you can control with code (Selenium).\n",
    "\n",
    "    This is used so each worker thread can load web pages quickly and quietly.\n",
    "    :return: A Selenium WebDriver object for Chrome. \n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    # Set Chrome options for headless operation and performance\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")  # Set a fixed window size \n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome without opening a window\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Needed for some server setups\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Fixes shared memory issues\n",
    "\n",
    "    # Tell Chrome not to load images or show notifications\n",
    "    # This speeds up page loading and avoids pop-ups that can slow down scraping\n",
    "    # 'prefs' is a dictionary that sets Chrome's default content settings\n",
    "    prefs = {\n",
    "        \"profile.managed_default_content_settings.images\": 2,  # Block images\n",
    "        \"profile.managed_default_content_settings.stylesheets\": 2,  # Block stylesheets\n",
    "        \"profile.managed_default_content_settings.scripts\": 2,  # Block scripts\n",
    "        \"profile.managed_default_content_settings.popups\": 2,  # Block pop-ups\n",
    "        \"profile.default_content_setting_values.notifications\": 2,  # Block pop-ups\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "\n",
    "    # Start Chrome with these settings and return the driver object\n",
    "    return webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792bde6",
   "metadata": {},
   "source": [
    "# The “worker” function\n",
    "\n",
    "Think of a worker as a little helper that does the following, over and over:\n",
    "\n",
    "1. **Opens its own invisible browser** (so it can visit websites without you seeing anything).\n",
    "2. **Takes a website address** from a shared list (the queue).\n",
    "3. **Tries to load the page** (waits up to 2 seconds—if it’s too slow, skips it).\n",
    "4. **Reads the page’s HTML** using BeautifulSoup (so it can look for clues).\n",
    "5. **Uses the FeaturesExtraction tool** to turn the page into a list of numbers (features) that describe it.\n",
    "6. **Adds the website address and its label** (1 = phishing, 0 = legit) to the end of that list.\n",
    "7. **Writes that row into a CSV file** right away (using a lock so two helpers don’t write at the same time).\n",
    "8. **Keeps track of how many pages it’s finished** and prints the count as it goes.\n",
    "9. **Repeats this process** for each website, until it gets a special “None” signal (which means “you’re done!”).\n",
    "10. **Closes its browser** before stopping.\n",
    "\n",
    "**Why use multiple workers?**  \n",
    "By running several workers at once (each in its own thread), the script can process many websites in parallel, making the whole job much faster. Each worker is independent and safe from interfering with others, thanks to the queue (for sharing work) and the lock (for writing to the file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27591e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cest bon\n"
     ]
    }
   ],
   "source": [
    "def worker(queue, output_file, label, columns_lock):\n",
    "    \"\"\"\n",
    "    Worker function to be run in each thread.\n",
    "    - queue: shared Queue of URLs to process\n",
    "    - output_file: CSV path to append features\n",
    "    - label: 1 for phishing, 0 for legitimate\n",
    "    - columns_lock: threading.Lock to synchronize file writes\n",
    "    \"\"\"\n",
    "    # initialize per-thread counters\n",
    "    phishing_count.value = 0\n",
    "    legitimate_count.value = 0\n",
    "\n",
    "    driver = initialize_driver()  # get a fresh WebDriver\n",
    "    driver.set_page_load_timeout(2)  # set max page load time to 2 seconds\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            url = queue.get()  # get next URL (blocks if empty)\n",
    "            if url is None:  # termination signal\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                driver.get(url)  # navigate to the URL\n",
    "                soup = BeautifulSoup(\n",
    "                    driver.page_source, \"html.parser\"\n",
    "                )  # parse page HTML\n",
    "\n",
    "                # extract features from URL and DOM\n",
    "                extractor = FeaturesExtraction(driver, soup, url)\n",
    "                vector = extractor.create_vector()  # list of feature values\n",
    "\n",
    "                # append metadata: URL string and label\n",
    "                vector.append(str(url))\n",
    "                vector.append(label)\n",
    "\n",
    "                # thread-safe write: convert to DataFrame and append as a row\n",
    "                with columns_lock:\n",
    "                    df_row = pd.DataFrame([vector], columns=get_feature_columns())\n",
    "                    df_row.to_csv(output_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "                # update and display counters\n",
    "                if label == 1:\n",
    "                    phishing_count.value += 1\n",
    "                    print(f\"\\rProcessed phishing sites: {phishing_count.value}\", end=\"\")\n",
    "                elif label == 0:\n",
    "                    legitimate_count.value += 1\n",
    "                    print(\n",
    "                        f\"\\rProcessed legitimate sites: {legitimate_count.value}\",\n",
    "                        end=\"\",\n",
    "                    )\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout while loading URL: {url}\")  # page took too long\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")  # catch-all for other errors\n",
    "            finally:\n",
    "                queue.task_done()  # signal that this URL is processed\n",
    "    finally:\n",
    "        driver.quit()  # ensure browser is closed when thread finishes\n",
    "        \n",
    "print(\"cest bon\")  # indicate thread completion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d7d42",
   "metadata": {},
   "source": [
    "# Orchestration Function Explained\n",
    "\n",
    "This function is the \"conductor\" that coordinates the entire data collection process. Here’s what it does, step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Read a slice from the input CSV**\n",
    "\n",
    "- **Phishing URLs:**  \n",
    "    If you’re processing phishing sites, it reads a chunk of rows from a CSV file (like `verified_online.csv`). It grabs the `url` column, which contains the full web addresses to visit.\n",
    "\n",
    "- **Legitimate URLs:**  \n",
    "    If you’re processing legitimate sites, it reads a chunk of rows from a CSV file (like `tranco_list.csv`). These rows usually just have domain names (like `google.com`). The function adds `http://` in front to make them full URLs.\n",
    "\n",
    "- **Why a slice?**  \n",
    "    You can specify which rows to process (`start_row` and `end_row`). This lets you break up a huge list into smaller, manageable pieces.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Build the list of URLs**\n",
    "\n",
    "- After reading the CSV, it creates a Python list of URLs to visit.\n",
    "- For legitimate domains, it automatically adds `http://` to each domain name.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Initialize a Queue, start worker threads, and ensure the output CSV has headers**\n",
    "\n",
    "- **Queue:**  \n",
    "    A thread-safe queue is created to hold all the URLs. This lets multiple worker threads safely pull URLs to process, one at a time.\n",
    "\n",
    "- **Worker Threads:**  \n",
    "    The function starts several worker threads (mini-programs running in parallel). Each worker:\n",
    "    - Opens its own invisible browser.\n",
    "    - Visits URLs from the queue.\n",
    "    - Extracts features from the page.\n",
    "    - Writes results to the output CSV.\n",
    "\n",
    "- **Output CSV:**  \n",
    "    If the output file (where results are saved) doesn’t exist yet, the function writes a header row with all the feature names. This ensures the CSV is ready for new data.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Enqueue URLs with a progress bar**\n",
    "\n",
    "- All URLs are added to the queue, one by one.\n",
    "- A progress bar (using `tqdm`) shows how many URLs have been queued, so you can track progress.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Signal threads to stop and wait for them**\n",
    "\n",
    "- After all URLs are queued, the function adds a special `None` value to the queue for each worker.  \n",
    "    This tells each worker, “You’re done, you can stop now.”\n",
    "- The function waits for all worker threads to finish before exiting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is this important?**\n",
    "\n",
    "- **Parallel Processing:**  \n",
    "    By using multiple threads, the function can process many URLs at the same time, making the whole process much faster.\n",
    "\n",
    "- **Thread Safety:**  \n",
    "    The queue and file-writing lock prevent data corruption when multiple threads are running.\n",
    "\n",
    "- **Scalability:**  \n",
    "    You can adjust the number of threads and the size of the data slice to fit your computer’s resources.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Terms**\n",
    "\n",
    "- **Queue:**  \n",
    "    A special list that lets threads safely share work.\n",
    "\n",
    "- **Worker Thread:**  \n",
    "    A mini-program that does the actual work (visiting URLs, extracting features).\n",
    "\n",
    "- **CSV:**  \n",
    "    A file format for storing tabular data (like a spreadsheet).\n",
    "\n",
    "- **Progress Bar:**  \n",
    "    A visual indicator of how much work has been done.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "This orchestration function reads a batch of URLs, sets up parallel workers, manages safe data sharing, and writes the extracted features to a CSV file for later machine learning or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f382c5e",
   "metadata": {},
   "source": [
    " ### Orchestrating everything (`process_urls_threaded`)\n",
    "\n",
    "1. **Read** the input CSV (either phishing or legitimate list) for the slice of rows you asked for.\n",
    "2. **Build** a Python list of URLs.\n",
    "3. **Prepare**:\n",
    "\n",
    "   * A `Queue` loaded with all those URLs.\n",
    "   * A `Lock` to guard file writes.\n",
    "4. **Create** the output CSV with headers if it doesn’t already exist.\n",
    "5. **Launch** N worker threads, each running the `worker` function.\n",
    "6. **Feed** each URL into the queue (with a nice progress bar).\n",
    "7. **After** all URLs are queued, put N `None` entries to tell each worker “you’re done.”\n",
    "8. **Wait** for all the threads to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95aea0",
   "metadata": {},
   "source": [
    "# Cell 5: Column Names\n",
    "\n",
    "**Purpose:** Returns the ordered list of all feature names + `URL` and `label`.  \n",
    "Must exactly match the order produced by `FeaturesExtraction.create_vector()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44292df6",
   "metadata": {},
   "source": [
    "## 6. Defining the column names (`get_feature_columns`)\n",
    "\n",
    "* This function just returns a long list of all the names of the clues (features) in the exact order that `FeaturesExtraction` \n",
    "produces them, plus two extras at the end:\n",
    "\n",
    "  * **URL** (so you can see which row is which page)\n",
    "  * **label** (1 or 0, telling you phishing or legitimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "    \"\"\"\n",
    "    Generate the ordered list of feature column names for the output CSV.\n",
    "    Returns a list of strings: all HTML, dynamic, password, JS, URL-based features, plus metadata.\n",
    "    \"\"\"\n",
    "    # List of basic HTML and page structure features\n",
    "    basic_columns = [\n",
    "        \"has_title\",                  # Whether the page has a <title> tag\n",
    "        \"has_submit\",                 # Whether the page has a submit button\n",
    "        \"has_link\",                   # Whether the page has any <a> links\n",
    "        \"has_email_input\",            # Whether the page has an email input field\n",
    "        \"number_of_inputs\",           # Number of input fields on the page\n",
    "        \"number_of_buttons\",          # Number of button elements\n",
    "        \"number_of_images\",           # Number of image elements\n",
    "        \"number_of_option\",           # Number of <option> elements (dropdowns)\n",
    "        \"number_of_list\",             # Number of list elements (<ul>, <ol>)\n",
    "        \"number_of_href\",             # Number of href attributes (links)\n",
    "        \"number_of_paragraph\",        # Number of <p> (paragraph) elements\n",
    "        \"number_of_script\",           # Number of <script> tags\n",
    "        \"length_of_title\",            # Length of the page title\n",
    "        \"has_h1\",                     # Whether the page has an <h1> header\n",
    "        \"has_h2\",                     # Whether the page has an <h2> header\n",
    "        \"has_h3\",                     # Whether the page has an <h3> header\n",
    "        \"length_of_text\",             # Total length of visible text\n",
    "        \"number_of_clickable_button\", # Number of clickable buttons\n",
    "        \"number_of_a\",                # Number of <a> tags\n",
    "        \"number_of_div\",              # Number of <div> elements\n",
    "        \"has_footer\",                 # Whether the page has a <footer>\n",
    "        \"number_of_forms\",            # Number of <form> elements\n",
    "        \"has_text_area\",              # Whether the page has a <textarea>\n",
    "        \"has_iframe\",                 # Whether the page has an <iframe>\n",
    "        \"has_text_input\",             # Whether the page has a text input\n",
    "        \"number_of_meta\",             # Number of <meta> tags\n",
    "        \"has_nav\",                    # Whether the page has a <nav> element\n",
    "        \"number_of_sources\",          # Number of <source> tags (media)\n",
    "        \"number_of_span\",             # Number of <span> elements\n",
    "        \"number_of_table\",            # Number of <table> elements\n",
    "        \"RequestURL\",                 # Feature related to request URLs\n",
    "        \"AnchorURL\",                  # Feature related to anchor URLs\n",
    "        \"Favicon\",                    # Whether the page has a favicon\n",
    "        \"LinksInScriptTags\",          # Links found inside <script> tags\n",
    "        \"ServerFormHandler\",          # Whether the form is handled by the server\n",
    "        \"InfoEmail\",                  # Presence of info email addresses\n",
    "    ]\n",
    "\n",
    "    # Features related to dynamic or interactive content\n",
    "    dynamic_columns = [\n",
    "        \"has_mouse_tracking\",         # Whether mouse tracking is present\n",
    "        \"has_keyboard_monitoring\",    # Whether keyboard monitoring is present\n",
    "        \"has_popups\",                 # Whether popups are present\n",
    "        \"number_of_hidden_element\",   # Number of hidden elements\n",
    "        \"page_redirect\",              # Whether the page redirects\n",
    "        \"form_redirect_behavior\",     # Form submission causes redirect\n",
    "        \"external_form_action\",       # Form action points to external site\n",
    "    ]\n",
    "\n",
    "    # Features related to password fields and forms\n",
    "    password_columns = [\n",
    "        \"password_type_count\",        # Number of password input types\n",
    "        \"password_name_id_count\",     # Number of password fields with name/id\n",
    "        \"hidden_password_count\",      # Number of hidden password fields\n",
    "        \"form_with_password\",         # Whether a form contains a password field\n",
    "    ]\n",
    "\n",
    "    # Features related to JavaScript activity\n",
    "    js_columns = [\n",
    "        \"clipboard_monitoring\",       # JS monitors clipboard\n",
    "        \"form_data_collection\",       # JS collects form data\n",
    "        \"cookie_manipulation\",        # JS manipulates cookies\n",
    "    ]\n",
    "\n",
    "    # Features extracted from the URL or domain\n",
    "    url_features_columns = [\n",
    "        \"UsingIp\",                    # URL uses an IP address\n",
    "        \"longUrl\",                    # URL is long\n",
    "        \"shortUrl\",                   # URL is short\n",
    "        \"symbol\",                     # URL contains special symbols\n",
    "        \"redirecting\",                # URL redirects\n",
    "        \"prefixSuffix\",               # URL has prefix/suffix\n",
    "        \"SubDomains\",                 # Number of subdomains\n",
    "        \"Hppts\",                      # Misspelled HTTPS\n",
    "        \"DomainRegLen\",               # Domain registration length\n",
    "        \"NonStdPort\",                 # Uses non-standard port\n",
    "        \"HTTPSDomainURL\",             # HTTPS in domain part\n",
    "        \"AbnormalURL\",                # Abnormal URL structure\n",
    "        \"WebsiteForwarding\",          # Website forwarding detected\n",
    "        \"StatusBarCust\",              # Custom status bar\n",
    "        \"DisableRightClick\",          # Right-click disabled\n",
    "        \"UsingPopupWindow\",           # Uses popup windows\n",
    "        \"IframeRedirection\",          # Uses iframe for redirection\n",
    "        \"AgeofDomain\",                # Age of the domain\n",
    "        \"DNSRecording\",               # DNS record presence\n",
    "        \"WebsiteTraffic\",             # Website traffic rank\n",
    "        \"PageRank\",                   # PageRank value\n",
    "        \"GoogleIndex\",                # Indexed by Google\n",
    "        \"LinksPointingToPage\",        # Number of links pointing to page\n",
    "        \"StatsReport\",                # Statistical report presence\n",
    "    ]\n",
    "\n",
    "    # Metadata columns (URL and label/class)\n",
    "    metadata_columns = [\n",
    "        \"URL\",                        # The URL of the page\n",
    "        \"label\",                      # The label/class (e.g., phishing/legit)\n",
    "    ]\n",
    "\n",
    "    # Combine all feature lists into a single list and return it\n",
    "    return (\n",
    "        basic_columns\n",
    "        + dynamic_columns\n",
    "        + password_columns\n",
    "        + js_columns\n",
    "        + url_features_columns\n",
    "        + metadata_columns\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f0346",
   "metadata": {},
   "source": [
    "## The “main” section\n",
    "\n",
    "When you run the script directly:\n",
    "\n",
    "1. It sets how many threads to use for phishing and legitimate sites.\n",
    "2. It picks which rows of each CSV to process (so you can do them in chunks).\n",
    "3. It **only** processes the legitimate list (the phishing part is commented out, but you could turn it on).\n",
    "4. You see “Processing legitimate websites…” then the progress bar.\n",
    "5. At the end, you see “Processing completed!”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3800902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    # This block only runs if the script is executed directly (not imported as a module).\n",
    "\n",
    "    # Set how many threads to use for each type of URL processing:\n",
    "    PHISH_THREADS = 32              # Number of parallel threads for phishing URLs\n",
    "    LEGITIMATE_THRESHOLD = 16       # Number of parallel threads for legitimate URLs\n",
    "\n",
    "    # Define which rows of the phishing CSV to process (for batching):\n",
    "    start_row_phishing = 21000      # Start at row 21,000 in the phishing CSV\n",
    "    end_row_phishing = start_row_phishing + 1000  # Process 1,000 rows (21,000–21,999)\n",
    "\n",
    "    # Define which rows of the legitimate CSV to process:\n",
    "    start_row_legitimate = 24100    # Start at row 24,100 in the legitimate CSV\n",
    "    end_row_legitimate = start_row_legitimate + 200  # Process 200 rows (24,100–24,299)\n",
    "''' \n",
    "    # --- PHISHING PROCESSING (currently commented out) ---\n",
    "    # To process phishing URLs, uncomment the following lines:\n",
    "    # print(\"Processing phishing websites...\")\n",
    "    # process_urls_threaded(\n",
    "    #     \"verified_online.csv\",         # Input CSV with phishing URLs\n",
    "    #     \"phishing_websites.csv\",       # Output CSV for extracted features\n",
    "    #     label=1,                       # Label 1 = phishing\n",
    "    #     start_row=start_row_phishing,  # Start row for batch\n",
    "    #     end_row=end_row_phishing,      # End row for batch\n",
    "    #     num_threads=PHISH_THREADS,     # Number of threads to use\n",
    "    # )\n",
    "\n",
    "    # # --- LEGITIMATE PROCESSING (active) ---\n",
    "    # print(\"Processing legitimate websites...\")\n",
    "    # process_urls_threaded(\n",
    "    #     \"tranco_list.csv\",               # Input CSV with legitimate domains\n",
    "    #     \"legitimate_websites.csv\",       # Output CSV for extracted features\n",
    "    #     label=0,                         # Label 0 = legitimate\n",
    "    #     start_row=start_row_legitimate,  # Start row for batch\n",
    "    #     end_row=end_row_legitimate,      # End row for batch\n",
    "    #     num_threads=LEGITIMATE_THRESHOLD,# Number of threads to use\n",
    "    # )\n",
    "'''\n",
    "print(\"Processing completed!\")       # Print when all URLs are processed\n",
    "\n",
    "# In summary:\n",
    "# - This code sets up how many threads to use and which rows to process from each input file.\n",
    "# - It can process phishing or legitimate URLs (one at a time, depending on which block is active).\n",
    "# - It calls process_urls_threaded() to extract features from each URL and save them to a CSV.\n",
    "# - The process is parallelized for speed, and you can control the batch size by changing the row ranges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
