{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0f7631",
   "metadata": {},
   "source": [
    "# 1. Imports & Global State\n",
    "\n",
    "**Purpose:**  \n",
    "Load all required libraries and set up shared variables:\n",
    "\n",
    "- **Concurrency:** `threading` + `Queue` let us run many workers in parallel without stepping on each other’s toes.  \n",
    "- **Browser Automation:** `selenium` + headless Chrome to render pages (including JavaScript).  \n",
    "- **HTML Parsing:** `BeautifulSoup` to inspect the final DOM.  \n",
    "- **Data I/O:** `pandas` for CSV read/write; `tqdm` for progress bars.  \n",
    "- **Feature Extraction:** our custom `FeaturesExtraction` class.  \n",
    "- **Counters & Lock:** global counters to track how many phishing vs legitimate URLs we’ve processed, plus a `lock` to serialize file writes and counter updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2f9f2",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "Purpose:\n",
    "  Evaluate multithreaded URL feature extraction performance.\n",
    "  Reads URLs, launches headless browsers in threads,\n",
    "  extracts features, limits to max_count per label, and measures time.\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56257814",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'features_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mselenium\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeoutException  \u001b[38;5;66;03m# Handle slow page loads\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m                            \u001b[38;5;66;03m# Read/write CSVs and DataFrame handling\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeatures_extraction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturesExtraction  \u001b[38;5;66;03m# Extract feature vectors from pages\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InsecureRequestWarning  \u001b[38;5;66;03m# Suppress SSL warnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_warnings            \u001b[38;5;66;03m# Disable those warnings globally\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'features_extraction'"
     ]
    }
   ],
   "source": [
    "\n",
    "import threading                               # For concurrent worker threads\n",
    "from queue import Queue                        # Thread-safe queue of URLs\n",
    "from bs4 import BeautifulSoup                  # Parse HTML page source\n",
    "from selenium import webdriver                 # Automate Chrome to render pages\n",
    "from selenium.webdriver.chrome.options import Options  # Configure ChromeDriver\n",
    "from selenium.common.exceptions import TimeoutException  # Handle slow page loads\n",
    "import pandas as pd                            # Read/write CSVs and DataFrame handling\n",
    "from features_extraction import FeaturesExtraction  # Extract feature vectors from pages\n",
    "from urllib3.exceptions import InsecureRequestWarning  # Suppress SSL warnings\n",
    "from urllib3 import disable_warnings            # Disable those warnings globally\n",
    "import time                                    # Track elapsed time for performance\n",
    "from tqdm import tqdm                          # Display progress bars\n",
    "\n",
    "disable_warnings(InsecureRequestWarning)       # Turn off insecure request warnings\n",
    "\n",
    "# Global counters for processed pages, protected by lock\n",
    "phishing_count = 0                             # Total phishing pages processed\n",
    "legitimate_count = 0                           # Total legitimate pages processed\n",
    "lock = threading.Lock()                        # Mutex to synchronize shared state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a4d53",
   "metadata": {},
   "source": [
    "# 2. Headless Chrome Setup\n",
    "\n",
    "**Purpose:**  \n",
    "Every worker needs its own Chrome browser instance. This helper:\n",
    "\n",
    "1. Runs Chrome **headless** (no window).  \n",
    "2. Disables sandboxing & shared‑memory mode for compatibility in Docker/CI.  \n",
    "3. Turns off images & notifications for speed.  \n",
    "\n",
    "By isolating these options in one function, the worker code stays clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    \"\"\"\n",
    "    Create and return a headless Chrome WebDriver optimized for speed.\n",
    "    \"\"\"\n",
    "    # Configure headless Chrome options\n",
    "    chrome_options = Options()                  # Initialize options container\n",
    "    chrome_options.add_argument(\"--headless\")  # Do not open browser window\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Required in some Linux environments\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Avoid shared memory errors\n",
    "    prefs = {                                   # Browser preferences\n",
    "        \"profile.managed_default_content_settings.images\": 2,  # Disable images\n",
    "        \"profile.default_content_setting_values.notifications\": 2  # Disable notifications\n",
    "    }\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)  # Apply preferences\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)  # Launch Chrome WebDriver\n",
    "\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cc0f1",
   "metadata": {},
   "source": [
    "# 3. Worker Logic\n",
    "\n",
    "**Purpose:**  \n",
    "Each thread runs this loop:\n",
    "\n",
    "1. **Pull** a URL from the shared `queue`.  \n",
    "2. **Stop** when it receives a `None` sentinel.  \n",
    "3. **Skip** if that class already reached `max_count`.  \n",
    "4. **Load** the page (2 s timeout).  \n",
    "5. **Parse** the rendered HTML via BeautifulSoup.  \n",
    "6. **Extract** numeric features via `FeaturesExtraction`.  \n",
    "7. **Append** the URL and class label to the feature vector.  \n",
    "8. **Write** one CSV row under `lock` so rows never interleave.  \n",
    "9. **Update** the appropriate counter and print live progress.  \n",
    "10. **Handle** timeouts by restarting the driver, and always `quit()` at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(queue, data_list, label, max_count):\n",
    "    global phishing_count, legitimate_count\n",
    "\n",
    "    driver = initialize_driver()\n",
    "    driver.set_page_load_timeout(2)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            url = queue.get()                   # Fetch next URL or None\n",
    "            if url is None:                     # Termination signal\n",
    "                break\n",
    "\n",
    "            with lock:                          # Protect counters\n",
    "                # Skip if reached max_count for this label\n",
    "                if (label == 1 and phishing_count >= max_count) or \\\n",
    "                   (label == 0 and legitimate_count >= max_count):\n",
    "                    queue.task_done()            # Mark task done\n",
    "                    continue                     # Skip processing\n",
    "\n",
    "            print(f\"\\nProcessing URL: {url}\")  # Log URL\n",
    "            try:\n",
    "                driver.get(url)                 # Load page in browser\n",
    "                print(f\"Successfully loaded URL: {url}\")\n",
    "\n",
    "                # Parse rendered HTML and extract features\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                extractor = FeaturesExtraction(driver, soup, url)\n",
    "                vector = extractor.create_vector()  # Feature list\n",
    "                vector.append(str(url))            # Append URL metadata\n",
    "                vector.append(label)               # Append class label\n",
    "\n",
    "                with lock:                        # Synchronize shared list and counters\n",
    "                    if label == 1 and phishing_count < max_count:\n",
    "                        data_list.append(vector)   # Store phishing vector\n",
    "                        phishing_count += 1        # Increment phishing count\n",
    "                        print(f\"\\rProcessed phishing sites: {phishing_count}/{max_count}\", end=\"\")\n",
    "                    elif label == 0 and legitimate_count < max_count:\n",
    "                        data_list.append(vector)   # Store legitimate vector\n",
    "                        legitimate_count += 1      # Increment legitimate count\n",
    "                        print(f\"\\rProcessed legitimate sites: {legitimate_count}/{max_count}\", end=\"\")\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout while loading URL: {url}\")\n",
    "                driver.quit()                    # Restart driver on timeout\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {str(e)}\")\n",
    "                continue                          # Skip on error\n",
    "            finally:\n",
    "                queue.task_done()                # Signal completion\n",
    "    finally:\n",
    "        driver.quit()                          # Clean up browser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030f2f0",
   "metadata": {},
   "source": [
    "# 4. Orchestrator: process_urls_threaded()\n",
    "\n",
    "**Purpose:**  \n",
    "Wrap everything together:\n",
    "\n",
    "1. **Reset** the global counters for the chosen label.  \n",
    "2. **Read** a slice of URLs from the phishing CSV (label 1) or Tranco CSV (label 0).  \n",
    "3. **Initialize** the URL queue, in‑memory list, and worker threads.  \n",
    "4. **Enqueue** URLs (up to `max_count`), showing a progress bar.  \n",
    "5. **Send** `None` to stop threads, then `join()` them.  \n",
    "6. **Build** a final DataFrame from `data_list` and save it as a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7826091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_urls_threaded(input_file, output_file, label, num_threads=4, max_count=100):\n",
    "    global phishing_count, legitimate_count\n",
    "\n",
    "    # Reset counters based on label\n",
    "    if label == 1:\n",
    "        phishing_count = 0\n",
    "    else:\n",
    "        legitimate_count = 0\n",
    "\n",
    "    # Read input list of URLs\n",
    "    if label == 0:\n",
    "        df = pd.read_csv(input_file, names=[\"id\", \"url\"])  # No header\n",
    "        URL_list = [\"http://\" + u for u in df['url'].tolist()]\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)             # CSV with header\n",
    "        URL_list = df['url'].tolist()\n",
    "\n",
    "    url_queue = Queue()                         # Shared URL queue\n",
    "    data_list = []                              # Collect feature vectors\n",
    "\n",
    "    # Spawn worker threads\n",
    "    threads = []                                # Keep references to threads\n",
    "    for _ in range(num_threads):\n",
    "        t = threading.Thread(target=worker, args=(url_queue, data_list, label, max_count))\n",
    "        t.daemon = True                          # Daemon threads exit with main program\n",
    "        t.start()                                # Begin execution\n",
    "        threads.append(t)\n",
    "\n",
    "    # Enqueue URLs until max_count reached\n",
    "    with tqdm(total=max_count, desc=\"Processing URLs\") as pbar:\n",
    "        for url in URL_list:\n",
    "            with lock:\n",
    "                if (label == 1 and phishing_count >= max_count) or \\\n",
    "                   (label == 0 and legitimate_count >= max_count):\n",
    "                    break                          # Stop adding URLs\n",
    "            url_queue.put(url)                    # Add URL to queue\n",
    "            # Update progress by current count\n",
    "            pbar.update(phishing_count if label == 1 else legitimate_count)\n",
    "\n",
    "    # Signal threads to terminate\n",
    "    for _ in range(num_threads):\n",
    "        url_queue.put(None)\n",
    "    for t in threads:\n",
    "        t.join()                                 # Wait for all threads to finish\n",
    "\n",
    "    # 6) Write final CSV\n",
    "     df_out = pd.DataFrame(data=data_list, columns=columns)  # Create DataFrame\n",
    "    df_out.to_csv(output_file, index=False)      # Save to CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3b09c",
   "metadata": {},
   "source": [
    "# 5. Feature Column Names\n",
    "\n",
    "**Purpose:**  \n",
    "Return the exact ordered list of column names used in the CSV:\n",
    "\n",
    "- HTML static features  \n",
    "- Dynamic (JS) features  \n",
    "- Password‐field features  \n",
    "- Suspicious JS features  \n",
    "- URL‐based features  \n",
    "- Finally: `URL`, `label`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143105a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataFrame header\n",
    "    def get_feature_columns():\n",
    "        basic = [  # HTML static\n",
    "            \"has_title\",\"has_submit\",\"has_link\",\"has_email_input\",\n",
    "            \"number_of_inputs\",\"number_of_buttons\",\"number_of_images\",\n",
    "            \"number_of_option\",\"number_of_list\",\"number_of_href\",\n",
    "            \"number_of_paragraph\",\"number_of_script\",\"length_of_title\",\n",
    "            \"has_h1\",\"has_h2\",\"has_h3\",\"length_of_text\",\n",
    "            \"number_of_clickable_button\",\"number_of_a\",\"number_of_div\",\n",
    "            \"has_footer\",\"number_of_forms\",\"has_text_area\",\"has_iframe\",\n",
    "            \"has_text_input\",\"number_of_meta\",\"has_nav\",\n",
    "            \"number_of_sources\",\"number_of_span\",\"number_of_table\"\n",
    "        ]\n",
    "        dynamic = [  # Dynamic JS\n",
    "            \"has_mouse_tracking\",\"has_keyboard_monitoring\",\n",
    "            \"has_popups\",\"number_of_hidden_element\",\n",
    "            \"page_redirect\",\"form_redirect_behavior\",\"external_form_action\"\n",
    "        ]\n",
    "        password = [  # Password features\n",
    "            \"password_type_count\",\"password_name_id_count\",\n",
    "            \"hidden_password_count\",\"form_with_password\"\n",
    "        ]\n",
    "        js_feats = [\"clipboard_monitoring\",\"form_data_collection\",\"cookie_manipulation\"]  # JS\n",
    "        metadata = [\"URL\",\"label\"]               # Metadata columns\n",
    "        return basic + dynamic + password + js_feats + metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb906cfd",
   "metadata": {},
   "source": [
    "# 6. Performance Testing & `__main__`\n",
    "\n",
    "**Purpose:**  \n",
    "- **`test_thread_performance()`**: Run the pipeline with different thread counts, measure total time, and return results.  \n",
    "- **`__main__` block**: When you `python this_script.py`, it tests `[10, 20, 50]` threads on the legitimate list and prints out the timings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test various thread counts\n",
    "def test_thread_performance(input_file, output_file_template, label, thread_counts):\n",
    "    results = []                                # Store (threads, time)\n",
    "    for nt in thread_counts:\n",
    "        print(f\"Testing with {nt} threads...\")\n",
    "        start_time = time.time()                # Start timer\n",
    "        process_urls_threaded(\n",
    "            input_file,\n",
    "            output_file_template.format(nt),\n",
    "            label,\n",
    "            num_threads=nt,\n",
    "            max_count=100\n",
    "        )\n",
    "        elapsed = time.time() - start_time      # Compute elapsed time\n",
    "        print(f\"Threads: {nt}, Time Taken: {elapsed:.2f} seconds\")\n",
    "        results.append((nt, elapsed))\n",
    "    return results\n",
    "\n",
    "# Entry point when run as script\n",
    "if __name__ == \"__main__\":\n",
    "    thread_counts_to_test = [10, 20, 50]        # Thread counts to evaluate\n",
    "    print(\"Testing legitimate websites processing speed...\")\n",
    "    legit_results = test_thread_performance(\n",
    "        \"tranco_list.csv\",\n",
    "        \"legitimate_websites_{}_threads.csv\",\n",
    "        label=0,\n",
    "        thread_counts=thread_counts_to_test\n",
    "    )\n",
    "    print(\"\\nResults:\")\n",
    "    for nt, elapsed in legit_results:\n",
    "        print(f\"Threads: {nt}, Time Taken: {elapsed:.2f} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
